{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Zimnat Insurance Recommendation Challenge Submission (Hichem5696)",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mD-IxLcq-TLr",
        "colab_type": "text"
      },
      "source": [
        "# First Thing to do\n",
        "* Upload Train.csv and Test.csv.\n",
        "* Activate GPU Runtime.\n",
        "* Preferably, get a Testla T4 GPU.\n",
        "\n",
        "When finished, run all cells below. The submission file will be generated at the end of the run (submission_stack.csv)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "agahWjIkilXJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install catboost\n",
        "!git clone --recursive https://github.com/Microsoft/LightGBM\n",
        "%cd /content/LightGBM\n",
        "!mkdir build\n",
        "!cmake -DUSE_GPU=1\n",
        "!make -j$(nproc)\n",
        "%cd /content/LightGBM/python-package\n",
        "!sudo python setup.py install --precompile\n",
        "%cd /content/\n",
        "!pip install xgboost --upgrade"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "grg282eL8gn_",
        "colab_type": "text"
      },
      "source": [
        "# Environement\n",
        "\n",
        "\n",
        "\n",
        "*   **Platform** : Google Colab GPU Runtime\n",
        "*   **GPU** : Tesla T4\n",
        "*   **Gradient Boost Algorithms used** : CatBoost (version 0.24.1), LightGBM (version 3.0.0.99) and XGBoost (version 1.2.0)\n",
        "*   **Estimated Runtime** : No more than 25 minutes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkifXHxefNVt",
        "colab_type": "text"
      },
      "source": [
        "# Methodology\n",
        "\n",
        "\n",
        "\n",
        "1.   Preparing the training and test set (using the DataPreparer class).\n",
        "2.   Split the training set into 2 parts : Set1 (80% of data) and Set2 (20% of data).\n",
        "3.   Train XGBoost, LightGBM and CatBoost on Set1.\n",
        "4.   Use Set2 to try to find the best weights to stack the 3 precedent models (using hyperopt library). This is a simple weighted average stacking.\n",
        "5.   Retrain the 3 models on all the training set and stack them using the weights found in Step 4.\n",
        "6.   Use the stacked model to make predictions on the test set.\n",
        "7.   Create the submission file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ScWnnxmnsmh",
        "colab_type": "text"
      },
      "source": [
        "# Preparation of Training and Test\n",
        "\n",
        "Given a row in the initial DataFrame, for every product that was purchased by the client, we treat it as it was not bought and let the model try to predict that it was in fact purchased."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Mrx-7IGnZVc",
        "colab_type": "text"
      },
      "source": [
        "# Features\n",
        "*   **Purchased Products (P5DA, RIBP, ...)** : It is a binary vector where a one means that the client purchased the product and a zero means he did not.\n",
        "*   **Sex** : Male or Female (Categorical Feature).\n",
        "*   **Marital status** : (Categorical Feature)\n",
        "*   **Branch code** : (Categorical Feature)\n",
        "*   **Occupation code** : (Categorical Feature)\n",
        "*   **Occupation category code** : (Categorical Feature)\n",
        "*   **Birth year**\n",
        "*   **date1** : Day number of the date when the client joined Zimnat\n",
        "*   **date3** : The year when the client joined Zimnat\n",
        "*   **date4** : The day name (Monday, Sunday, ...) of the date when the client joined Zimnat (Categorical Feature).\n",
        "*   **date_diff** : The age of the client when he joined Zimnat. It is equal to (date4 - Birth year).\n",
        "*   **num_products** : The number of products the client purchased (minus one because we removed a product that the model needs to predict).\n",
        "*   **popularity_score** : Represents the popularity of the products purchased by the client (The higher the more popular). It is calculated as follow:\n",
        "    1. Divide the **Purchased Products** matrix by its sum on axis=1 (for normalization purpose). The result is the matrix P.\n",
        "    2. Sum the matrix P on axis=0. The result is vector C.\n",
        "    3. Devide C by its sum.\n",
        "    4. Dot Product P @ C and the result is a popularity score for every instance in the dataset.\n",
        "*   **p_(Product_Code)** : Represent the popularity of the product (Product_Code) regarding **date3** feature. It is calculated as follow:\n",
        "    1. Regroup instances of Dataset by **date4** feature.\n",
        "    2. For each group, Sum the **Purchased Products** matrix and devide the resulting vector by its sum.\n",
        "    3. Join each vector with its corresponding group.\n",
        "*   **popularity_score_year** : This feature was supposed to be the equivalent of **popularity_score** but regarding the year (**date4** feature). Unfortunately, I did a bad manipulation while creating it which leave its value 0 for every instance of the Dataset. I will leave it as it is.\n",
        "\n",
        "**Note** : There were few join_date missing in the dataset. They were replaced by the mean date.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA8L3kjE7UQF",
        "colab_type": "text"
      },
      "source": [
        "# Dataset Preparer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cYwguiYkpMiX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import copy\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.base import TransformerMixin, BaseEstimator, RegressorMixin\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
        "\n",
        "def add_dummies(df, column_dummies, threshold=None):\n",
        "    \"\"\"Add dummies columns to a DataFrame. If threshold is not None, categories in a \n",
        "    column that represent less than threshold% of the data are regrouped in a\n",
        "    single category named '_rare__'\"\"\"\n",
        "    column_dummies = [x for x in column_dummies if x in df.columns]\n",
        "    for col in column_dummies:\n",
        "        if threshold:\n",
        "            t = df[col].value_counts(normalize=True) <= threshold\n",
        "            t = t[t].index.to_list()\n",
        "            df[col].replace(t, value='_rare__', inplace=True)\n",
        "        c = pd.get_dummies(df[col], prefix=col, dtype=np.int8)\n",
        "        df = pd.concat([df, c], axis=1)\n",
        "    df.drop(columns=column_dummies, inplace=True)\n",
        "    return df\n",
        "\n",
        "\n",
        "class DatasetPreparer:\n",
        "    def __init__(self, dummies=False, columns_drop=None, popularity_score=False,\n",
        "                 popularity_product_year=False, popularity_score_year=False):\n",
        "        \"\"\"This class prepares data for training, testing and submission.\n",
        "        dummies (bool): Whether to convert categorical columns to dummies or not.\n",
        "        columns_drop (list) : Columns to drop at the end of data preparation.\n",
        "        popularity_score (bool): Whether to include the popularity_score column.\n",
        "        popularity_product_year (bool): Whether to include the popularity_product_year column.\n",
        "        popularity_score_year (bool): Whether to include the popularity_score_year column.\n",
        "        \"\"\"\n",
        "\n",
        "        self.called = False\n",
        "        self.le = None\n",
        "        self.true_values = None\n",
        "        self.dummies = dummies\n",
        "        self.columns_drop = columns_drop\n",
        "        self.popularity_score = popularity_score\n",
        "        self.popularity_product_year = popularity_product_year\n",
        "        self.popularity_score_year = popularity_score_year\n",
        "\n",
        "    def get_train_test(self, train, test):\n",
        "        \"\"\"Preparation of training and test set\"\"\"\n",
        "        self.called = True\n",
        "        X_train = []\n",
        "        X_train_columns = train.columns\n",
        "        c = 0\n",
        "        for v in train.values:\n",
        "            info = v[:8]\n",
        "            binary = v[8:]\n",
        "            index = [k for k, i in enumerate(binary) if i == 1]\n",
        "            for i in index:\n",
        "                c += 1\n",
        "                for k in range(len(binary)):\n",
        "                    if k == i:\n",
        "                        binary_transformed = list(copy.copy(binary))\n",
        "                        binary_transformed[i] = 0\n",
        "                        X_train.append(list(info) + binary_transformed + [X_train_columns[8 + k]] + [c])\n",
        "\n",
        "        X_train = pd.DataFrame(X_train)\n",
        "        X_train.columns = ['ID', 'join_date', 'sex', 'marital_status', 'birth_year', 'branch_code',\n",
        "                           'occupation_code', 'occupation_category_code', 'P5DA', 'RIBP', '8NN1',\n",
        "                           '7POT', '66FJ', 'GYSR', 'SOP4', 'RVSZ', 'PYUQ', 'LJR9', 'N2MW', 'AHXO',\n",
        "                           'BSTQ', 'FM3X', 'K6QO', 'QBOL', 'JWFN', 'JZ9D', 'J9JW', 'GHYX', 'ECY3', 'product_pred',\n",
        "                           'ID2']\n",
        "\n",
        "        X_test = []\n",
        "        true_values = []\n",
        "        c = 0\n",
        "        for v in test.values:\n",
        "            c += 1\n",
        "            info = v[:8]\n",
        "            binary = v[8:]\n",
        "            index = [k for k, i in enumerate(binary) if i == 1]\n",
        "            X_test.append(list(info) + list(binary) + [c])\n",
        "            for k in test.columns[8:][index]:\n",
        "                true_values.append(v[0] + ' X ' + k)\n",
        "\n",
        "        X_test = pd.DataFrame(X_test)\n",
        "        X_test.columns = ['ID', 'join_date', 'sex', 'marital_status', 'birth_year', 'branch_code',\n",
        "                          'occupation_code', 'occupation_category_code', 'P5DA', 'RIBP', '8NN1',\n",
        "                          '7POT', '66FJ', 'GYSR', 'SOP4', 'RVSZ', 'PYUQ', 'LJR9', 'N2MW', 'AHXO',\n",
        "                          'BSTQ', 'FM3X', 'K6QO', 'QBOL', 'JWFN', 'JZ9D', 'J9JW', 'GHYX', 'ECY3', 'ID2']\n",
        "\n",
        "        features_train = []\n",
        "        features_test = []\n",
        "        columns = []\n",
        "\n",
        "        append_features = ['P5DA', 'RIBP', '8NN1', '7POT', '66FJ', 'GYSR', 'SOP4', 'RVSZ', 'PYUQ', 'LJR9',\n",
        "                           'N2MW', 'AHXO', 'BSTQ', 'FM3X', 'K6QO', 'QBOL', 'JWFN', 'JZ9D', 'J9JW', 'GHYX',\n",
        "                           'ECY3', 'ID', 'ID2', 'join_date', 'sex', 'marital_status', 'branch_code', 'occupation_code',\n",
        "                           'occupation_category_code',\n",
        "                           'birth_year']\n",
        "        for v in append_features:\n",
        "            features_train.append(X_train[v].values.reshape(-1, 1))\n",
        "            features_test.append(X_test[v].values.reshape(-1, 1))\n",
        "            columns.append(np.array([v]))\n",
        "\n",
        "        y_train = X_train[['product_pred']]\n",
        "\n",
        "        features_train = np.concatenate(features_train, axis=1)\n",
        "        features_test = np.concatenate(features_test, axis=1)\n",
        "        columns = np.concatenate(np.array(columns))\n",
        "\n",
        "        X_train = pd.DataFrame(features_train)\n",
        "        X_train.columns = columns\n",
        "        X_test = pd.DataFrame(features_test)\n",
        "        X_test.columns = columns\n",
        "\n",
        "        X_train['join_date'] = pd.to_datetime(X_train['join_date'])\n",
        "        X_test['join_date'] = pd.to_datetime(X_test['join_date'])\n",
        "\n",
        "        X_train['join_date'].fillna(X_train['join_date'].mean(), inplace=True)\n",
        "        X_test['join_date'].fillna(X_test['join_date'].mean(), inplace=True)\n",
        "\n",
        "        X_train['date1'] = X_train['join_date'].dt.day\n",
        "        X_train['date2'] = X_train['join_date'].dt.month\n",
        "        X_train['date3'] = X_train['join_date'].dt.year\n",
        "\n",
        "        X_test['date1'] = X_test['join_date'].dt.day\n",
        "        X_test['date2'] = X_test['join_date'].dt.month\n",
        "        X_test['date3'] = X_test['join_date'].dt.year\n",
        "\n",
        "        X_train['date_diff'] = X_train['date3'] - X_train['birth_year']\n",
        "        X_test['date_diff'] = X_test['date3'] - X_test['birth_year']\n",
        "\n",
        "        X_train['date4'] = X_train['join_date'].dt.day_name()\n",
        "        X_test['date4'] = X_test['join_date'].dt.day_name()\n",
        "\n",
        "        X_train.drop('join_date', axis=1, inplace=True)\n",
        "        X_test.drop('join_date', axis=1, inplace=True)\n",
        "\n",
        "        le = LabelEncoder()\n",
        "        data = X_train.append(X_test)\n",
        "        for v in ['sex', 'marital_status', 'branch_code', 'occupation_code', 'occupation_category_code', 'date4']:\n",
        "            data.loc[:, v] = le.fit_transform(data.loc[:, v])\n",
        "\n",
        "        if self.popularity_product_year:\n",
        "            def f(x):\n",
        "                x = x.iloc[:, :21].sum()\n",
        "                x /= x.sum()\n",
        "                return x\n",
        "\n",
        "            t = data.groupby(['date3']).apply(f)\n",
        "            t = t.add_prefix('p_')\n",
        "            data = data.join(t, on='date3')\n",
        "        if self.popularity_score_year:\n",
        "            data['popularity_score_year'] = 0\n",
        "            for d in data['date3'].unique():\n",
        "                c = data[data['date3'] == d]\n",
        "                w = c.iloc[:, :21].values.copy()\n",
        "                w = w / w.sum(axis=1, keepdims=True)\n",
        "                w2 = w.sum(axis=0)\n",
        "                w2 = w2 / w2.sum()\n",
        "                c['popularity_score_year'] = w @ w2\n",
        "        if self.dummies:\n",
        "            data = add_dummies(data, column_dummies=['sex', 'marital_status', 'branch_code', 'occupation_code',\n",
        "                                                     'occupation_category_code', 'date4'], threshold=0.001)\n",
        "        data['num_products'] = data.iloc[:, :21].sum(axis=1)\n",
        "\n",
        "        if self.popularity_score:\n",
        "            t = data.iloc[:, :21].values.copy()\n",
        "            t = t / t.sum(axis=1, keepdims=True)\n",
        "            t2 = t.sum(axis=0)\n",
        "            t2 = t2 / t2.sum()\n",
        "            data['popularity_score'] = t @ t2\n",
        "\n",
        "        X_train = data[:X_train.shape[0]]\n",
        "        X_test = data[-X_test.shape[0]:]\n",
        "\n",
        "        le.fit(y_train.iloc[:, 0])\n",
        "        y_train = pd.DataFrame(le.transform(y_train.iloc[:, 0]))\n",
        "        y_train.columns = ['target']\n",
        "\n",
        "        self.le = le\n",
        "        self.true_values = true_values\n",
        "\n",
        "        if self.columns_drop:\n",
        "            X_train.drop(columns=self.columns_drop, inplace=True)\n",
        "            X_test.drop(columns=self.columns_drop, inplace=True)\n",
        "\n",
        "        return X_train, X_test, y_train\n",
        "\n",
        "    def submission(self, model=None, X_test=None, prediction_proba=None):\n",
        "        \"\"\"Creating submission DataFrame\"\"\"\n",
        "        if not self.called:\n",
        "            raise RuntimeError('Run get_train_test first!')\n",
        "        if prediction_proba is None:\n",
        "            proba = model.predict_proba(X_test.drop(columns=['ID', 'ID2'], axis=1))\n",
        "        else:\n",
        "            proba = prediction_proba\n",
        "        y_test = pd.DataFrame(proba)\n",
        "        y_test.columns = self.le.inverse_transform(y_test.columns)\n",
        "\n",
        "        answer_mass = []\n",
        "        for i in range(X_test.shape[0]):\n",
        "            id = X_test['ID'].iloc[i]\n",
        "            for c in y_test.columns:\n",
        "                answer_mass.append([id + ' X ' + c, y_test[c].iloc[i]])\n",
        "\n",
        "        df_answer = pd.DataFrame(answer_mass)\n",
        "        df_answer.columns = ['ID X PCODE', 'Label']\n",
        "        for i in range(df_answer.shape[0]):\n",
        "            if df_answer['ID X PCODE'].iloc[i] in self.true_values:\n",
        "                df_answer['Label'].iat[i] = 1.0\n",
        "\n",
        "        df_answer.reset_index(drop=True, inplace=True)\n",
        "        return df_answer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "as3nb38K7DYV",
        "colab_type": "text"
      },
      "source": [
        "# Train models on Set1\n",
        "See Methodology for more information about what Set1 is.\n",
        "\n",
        "The hyperparameters we see here were found using a hyperparameter search library (hyperopt)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oAKnGJ31z5RB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from lightgbm.sklearn import LGBMClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "import xgboost as xgb\n",
        "\n",
        "params2 = {\n",
        "    'n_estimators': 6600,\n",
        "    'depth': 5,\n",
        "    'one_hot_max_size': 45,\n",
        "    'l2_leaf_reg': 8,\n",
        "    'task_type': 'GPU',\n",
        "    'loss_function': 'MultiClass',\n",
        "    'max_bin': 254\n",
        "}\n",
        "\n",
        "params = {\n",
        " 'n_estimators': 100,\n",
        " 'bagging_fraction': 0.4957961459694631,\n",
        " 'feature_fraction': 0.6037064857092298,\n",
        " 'lambda_l2': 0.21378633320033757,\n",
        " 'learning_rate': 0.08561797859363587,\n",
        " 'min_data_in_leaf': 100,\n",
        " 'num_leaves': 18}\n",
        "params['boosting_type'] = 'gbdt'\n",
        "params['objective'] = 'multiclass'\n",
        "params['metric'] = 'multi_logloss'\n",
        "params['num_class'] = 21\n",
        "params['device_type'] = 'gpu'\n",
        "params['verbose'] = -1\n",
        "\n",
        "\n",
        "params3 = { 'n_estimators': 149,\n",
        " 'colsample_bytree': 0.8506284701578654,\n",
        " 'eta': 0.12646057899961385,\n",
        " 'gamma': 0.03203531113764864,\n",
        " 'lambda': 2.863535707219281,\n",
        " 'max_depth': 6,\n",
        " 'min_child_weight': 1,\n",
        " 'subsample': 0.9645884644210851,\n",
        " 'objective': 'multi:softprob',\n",
        " 'eval_metric': 'mlogloss',\n",
        " 'num_class': 21,\n",
        " 'tree_method': 'gpu_hist'}\n",
        "\n",
        "models = [\n",
        "    xgb.XGBClassifier(**params3),\n",
        "    CatBoostClassifier(**params2),\n",
        "    LGBMClassifier(**params),\n",
        "]\n",
        "\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "\n",
        "dp = DatasetPreparer(dummies=False, columns_drop='date2', \n",
        "                     popularity_score_year=True, popularity_product_year=True, popularity_score=True)\n",
        "X_train, X_test, y_train = dp.get_train_test(train, test)\n",
        "dp_dummies = DatasetPreparer(dummies=True, columns_drop='date2', \n",
        "                             popularity_score_year=True, popularity_product_year=True, popularity_score=True)\n",
        "X_train_d, X_test_d, y_train_d = dp_dummies.get_train_test(train, test)\n",
        "\n",
        "\n",
        "# Class 8 is too rare so:\n",
        "# Oversample class 8 for cross validation to work\n",
        "X_train, y_train = X_train.reset_index(drop=True), y_train.reset_index(drop=True)\n",
        "t = X_train[y_train.target == 8]\n",
        "X_train = X_train.append(X_train.loc[t.index.repeat(2)].copy())\n",
        "y_train = y_train.target.append(pd.Series([8, ] * 8))\n",
        "# End of oversampling\n",
        "# Oversample class 8 for cross validation to work\n",
        "X_train_d, y_train_d = X_train_d.reset_index(drop=True), y_train_d.reset_index(drop=True)\n",
        "t = X_train_d[y_train_d.target == 8]\n",
        "X_train_d = X_train_d.append(X_train_d.loc[t.index.repeat(2)].copy())\n",
        "y_train_d = y_train_d.target.append(pd.Series([8, ] * 8))\n",
        "# End of oversampling\n",
        "\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify=y_train.values.ravel(), random_state=12345,\n",
        "                                          test_size=0.1)\n",
        "X_train_d, X_val_d, y_train_d, y_val_d = train_test_split(X_train_d, y_train_d, stratify=y_train_d.values.ravel(), random_state=12345,\n",
        "                                          test_size=0.1)\n",
        "\n",
        "X_train, X_val, X_test = X_train.infer_objects(), X_val.infer_objects(), X_test.infer_objects()\n",
        "X_train_d, X_val_d, X_test_d = X_train_d.infer_objects(), X_val_d.infer_objects(), X_test_d.infer_objects()\n",
        "for model in models:\n",
        "    if isinstance(model, CatBoostClassifier):\n",
        "        model.fit(X_train.drop(columns=['ID', 'ID2']), y_train.values.ravel(),\n",
        "              cat_features=['sex', 'marital_status', 'branch_code', 'occupation_code', 'occupation_category_code', 'date4'])\n",
        "    elif isinstance(model, LGBMClassifier):\n",
        "        model.fit(X_train.drop(columns=['ID', 'ID2']), y_train.values.ravel(),\n",
        "              categorical_feature=['sex', 'marital_status', 'branch_code', 'occupation_code', 'occupation_category_code', 'date4'])\n",
        "    elif isinstance(model, (xgb.XGBClassifier, Pipeline)):\n",
        "        model.fit(X_train_d.drop(columns=['ID', 'ID2']), y_train_d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zhbBCL5Mqws",
        "colab_type": "text"
      },
      "source": [
        "# Find weights for Weighted Average Stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_idEQo17TS3V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import os\n",
        "\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import cross_val_score, train_test_split\n",
        "from sklearn.metrics import log_loss\n",
        "from hyperopt import hp, Trials, fmin, anneal\n",
        "\n",
        "models_path = 'models_to_stack'\n",
        "features = []\n",
        "features_test = []\n",
        "for i, model in enumerate(models):\n",
        "    if isinstance(model, (xgb.XGBClassifier, Pipeline)):\n",
        "        t1 = pd.DataFrame(model.predict_proba(X_val_d.drop(columns=['ID', 'ID2', ]))).add_prefix(str(i) + '_')\n",
        "        t2 = pd.DataFrame(model.predict_proba(X_test_d.drop(columns=['ID', 'ID2']))).add_prefix(str(i) + '_')\n",
        "    elif isinstance(model, (LGBMClassifier, CatBoostClassifier)):\n",
        "        t1 = pd.DataFrame(model.predict_proba(X_val.drop(columns=['ID', 'ID2']))).add_prefix(str(i) + '_')\n",
        "        t2 = pd.DataFrame(model.predict_proba(X_test.drop(columns=['ID', 'ID2']))).add_prefix(str(i) + '_')\n",
        "    features.append(t1)\n",
        "    features_test.append(t2)\n",
        "\n",
        "print('Models losses :')\n",
        "for i, f in enumerate(features):\n",
        "    print('Model', i, ': ', log_loss(y_val, f))\n",
        "\n",
        "features = pd.concat(features, axis=1)\n",
        "features_test = pd.concat(features_test + [X_test[['ID', 'ID2']]], axis=1)\n",
        "\n",
        "def gb_cv(params, random_state=11837198, nfold=4, X=features, y=y_val):\n",
        "    X = X.values.reshape(-1, len(models), 21).transpose((0,2,1))\n",
        "    weight = np.array([params[k] for k in params])\n",
        "    \n",
        "    X = (X * weight).sum(-1) / weight.sum()\n",
        "    return log_loss(y, X)\n",
        "\n",
        "# possible values of parameters\n",
        "space = {str(k) : hp.uniform(str(k), 0, 1) for k in range(len(models))}\n",
        "\n",
        "# trials will contain logging information\n",
        "trials = Trials()\n",
        "\n",
        "best = fmin(fn=gb_cv,  # function to optimize\n",
        "            space=space,\n",
        "            algo=anneal.suggest,\n",
        "            max_evals=500,\n",
        "            trials=trials,\n",
        "            rstate=np.random.RandomState(32465237)\n",
        "            )\n",
        "\n",
        "print(best)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_6jV735GRMl",
        "colab_type": "text"
      },
      "source": [
        "# Train on Full Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aqA4hG89uaBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare models for blending/stacking (5 CatBoostClassifier from best models hyperopt : not have to be the best ones)\n",
        "\n",
        "import pandas as pd\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "params2 = {\n",
        "    'n_estimators': 6600,\n",
        "    'depth': 5,\n",
        "    'one_hot_max_size': 45,\n",
        "    'l2_leaf_reg': 8,\n",
        "    'task_type': 'GPU',\n",
        "    'loss_function': 'MultiClass',\n",
        "    'max_bin': 254\n",
        "}\n",
        "\n",
        "params = {'bagging_fraction': 0.4957961459694631,\n",
        " 'feature_fraction': 0.6037064857092298,\n",
        " 'lambda_l2': 0.21378633320033757,\n",
        " 'learning_rate': 0.08561797859363587,\n",
        " 'min_data_in_leaf': 100,\n",
        " 'num_leaves': 18}\n",
        "params['boosting_type'] = 'gbdt'\n",
        "params['objective'] = 'multiclass'\n",
        "params['metric'] = 'multi_logloss'\n",
        "params['num_class'] = 21\n",
        "params['device_type'] = 'gpu'\n",
        "params['verbose'] = -1\n",
        "\n",
        "params3 = { 'n_estimators': 149,\n",
        " 'colsample_bytree': 0.8506284701578654,\n",
        " 'eta': 0.12646057899961385,\n",
        " 'gamma': 0.03203531113764864,\n",
        " 'lambda': 2.863535707219281,\n",
        " 'max_depth': 6,\n",
        " 'min_child_weight': 1,\n",
        " 'subsample': 0.9645884644210851,\n",
        " 'objective': 'multi:softprob',\n",
        " 'eval_metric': 'mlogloss',\n",
        " 'num_class': 21,\n",
        " 'tree_method': 'gpu_hist'}\n",
        "\n",
        "models = [\n",
        "    xgb.XGBClassifier(**params3),\n",
        "    CatBoostClassifier(**params2),\n",
        "    LGBMClassifier(**params),\n",
        "]\n",
        "\n",
        "train = pd.read_csv('Train.csv')\n",
        "test = pd.read_csv('Test.csv')\n",
        "\n",
        "dp = DatasetPreparer(dummies=False,columns_drop='date2', popularity_score_year=True, popularity_product_year=True, popularity_score=True)\n",
        "X_train, X_test, y_train = dp.get_train_test(train, test)\n",
        "dp_dummies = DatasetPreparer(dummies=True, columns_drop='date2', popularity_score_year=True, popularity_product_year=True, popularity_score=True)\n",
        "X_train_d, X_test_d, y_train_d = dp_dummies.get_train_test(train, test)\n",
        "\n",
        "X_train, X_test = X_train.infer_objects(), X_test.infer_objects()\n",
        "X_train_d, X_test_d = X_train_d.infer_objects(), X_test_d.infer_objects()\n",
        "\n",
        "for model in models:\n",
        "    if isinstance(model, CatBoostClassifier):\n",
        "        model.fit(X_train.drop(columns=['ID', 'ID2']), y_train.values.ravel(),\n",
        "              cat_features=['sex', 'marital_status', 'branch_code', 'occupation_code', 'occupation_category_code', 'date4'])\n",
        "    elif isinstance(model, LGBMClassifier):\n",
        "        model.fit(X_train.drop(columns=['ID', 'ID2']), y_train.values.ravel(),\n",
        "              categorical_feature=['sex', 'marital_status', 'branch_code', 'occupation_code', 'occupation_category_code', 'date4'])\n",
        "    elif isinstance(model, xgb.XGBClassifier):\n",
        "        model.fit(X_train_d.drop(columns=['ID', 'ID2']), y_train_d.values.ravel())\n",
        "\n",
        "\n",
        "features_test = []\n",
        "for i, model in enumerate(models):\n",
        "    if isinstance(model, xgb.XGBClassifier):\n",
        "        t2 = pd.DataFrame(model.predict_proba(X_test_d.drop(columns=['ID', 'ID2']))).add_prefix(str(i) + '_')\n",
        "    elif isinstance(model, (LGBMClassifier, CatBoostClassifier)):\n",
        "        t2 = pd.DataFrame(model.predict_proba(X_test.drop(columns=['ID', 'ID2']))).add_prefix(str(i) + '_')\n",
        "    features_test.append(t2)\n",
        "features_test = pd.concat(features_test, axis=1)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IczkzWvW459k",
        "colab_type": "text"
      },
      "source": [
        "# Weighted Average\n",
        "Using the weights found previously to stack the 3 models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "igwTbSyA42ok",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "proba = features_test.values.reshape(-1, len(models), 21).transpose((0,2,1))\n",
        "weights = best\n",
        "weights = [weights[str(i)] for i in range(len(models))]\n",
        "proba = (proba * weights).sum(-1) / sum(weights)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVr9wmOB5vQL",
        "colab_type": "text"
      },
      "source": [
        "# Submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEfpG5aE5xSy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "submission = dp.submission(X_test=X_test, prediction_proba=proba)\n",
        "submission.to_csv('submission_stack.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mP03H-0_xtuQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 380
        },
        "outputId": "2430e311-933d-4abc-8277-5e32e2dfe6ff"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sat Aug 29 09:36:01 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P0    33W /  70W |    415MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}